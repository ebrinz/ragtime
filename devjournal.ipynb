{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG / HyDE Example with Mistral Instruct 7b and Milvus DB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports dependencies and Mistral LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/crashy/.local/share/virtualenvs/ragtime-5o5M5Lb8/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:09<00:00,  3.09s/it]\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "would you lke a tic tac?\n",
      "\n",
      "No thanks, I don't eat sugary snacks.\n",
      "\n",
      "Would you like a sugar-free one instead?\n",
      "\n",
      "No, I don't eat snacks at\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "home_dir = os.getenv(\"HOME\")\n",
    "model_name = f'{home_dir}/ext-gits/Mistral-7B-Instruct-v0.3'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': tokenizer.eos_token})\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map={\"\": device},\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Test LLM\n",
    "\n",
    "prompt = \"would you lke a tic tac?\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        max_length=50,\n",
    "        num_return_sequences=1,\n",
    "        do_sample=True,\n",
    "        top_p=0.95,\n",
    "        top_k=50,\n",
    "        pad_token_id=tokenizer.pad_token_id\n",
    "    )\n",
    "\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34886\n",
      "Number of completely empty rows: 0\n",
      "Total lines in the file: 34887\n",
      "The longest plot is at index 26064 with length 36773 characters.\n",
      "Longest Plot: After a brief introduction to some of the main characters of the story, the beginning sees a group of Rishis, led by Vishvamitra, performing a Yajna in a forest not far from Ayodhya, the Capital of the Kingdom of Kosala. This Yajna, like several before it, is interrupted and destroyed by a group of flying demons led by Ravana's Mama(Uncle/Mother's Brother) Maricha. After seeing yet another Yajna destroyed, a despondent Vishvamitra appeals to Lord Vishnu for salvation. Vishnu appears in a spiritu...\n",
      "non_nan_rows_coun 34886\n",
      "nan_counts Series([], dtype: int64)\n",
      "                                                Plot\n",
      "0  A bartender is working at a saloon, serving dr...\n",
      "1  The moon, painted with a smiling face hangs ov...\n",
      "2  The film, just over a minute long, is composed...\n",
      "3  Lasting just 61 seconds and consisting of two ...\n",
      "4  The earliest known adaptation of the classic f...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "df = None\n",
    "# Load the dataset\n",
    "df = pd.read_csv('data/wiki_movie_plots_deduped.csv')\n",
    "\n",
    "print(df.shape[0])\n",
    "\n",
    "empty_rows = df[df.isnull().all(axis=1)]\n",
    "print(f\"Number of completely empty rows: {empty_rows.shape[0]}\")\n",
    "\n",
    "with open('data/wiki_movie_plots_deduped.csv', 'r') as file:\n",
    "    reader = csv.reader(file)\n",
    "    total_lines = sum(1 for row in reader)\n",
    "\n",
    "print(f\"Total lines in the file: {total_lines}\")\n",
    "\n",
    "# Ensure there are no NaNs in the 'Plot' column\n",
    "df = df[['Plot']].dropna()\n",
    "\n",
    "# Find the longest plot and its length\n",
    "longest_plot = df['Plot'].apply(len).idxmax()  # Find the index of the longest plot\n",
    "longest_plot_text = df['Plot'].iloc[longest_plot]  # Get the longest plot text\n",
    "longest_plot_length = len(longest_plot_text)  # Get the length of the longest plot text\n",
    "\n",
    "print(f\"The longest plot is at index {longest_plot} with length {longest_plot_length} characters.\")\n",
    "print(f\"Longest Plot: {longest_plot_text[:500]}...\")\n",
    "\n",
    "non_nan_rows_count = df.dropna().shape[0]\n",
    "print(\"non_nan_rows_coun\", non_nan_rows_count) # 34886\n",
    "\n",
    "# need to change logic to accomodate NaNs\n",
    "\n",
    "# Get the count of NaN values for each column\n",
    "nan_counts = df.isna().sum()\n",
    "nan_counts = nan_counts[nan_counts > 0]\n",
    "print(\"nan_counts\", nan_counts)\n",
    "\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "row_count = df.shape[0]\n",
    "row_count\n",
    "\n",
    "df = df.drop(df.index)\n",
    "\n",
    "row_count = df.shape[0]\n",
    "row_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Milvus connected: True\n",
      "Collection 'wiki_movie_plots' dropped.\n",
      "Milvus collection schema created successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1729629804.462455 6676211 fork_posix.cc:77] Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# run milvis locally\n",
    "# run: docker-compose up -d\n",
    "\n",
    "# data is ..data/wiki_movie_plots_deduped.csv is from\n",
    "# https://www.kaggle.com/datasets/jrobischon/wikipedia-movie-plots\n",
    "\n",
    "from pymilvus import connections, CollectionSchema, DataType, FieldSchema, Collection, utility\n",
    "\n",
    "connections.connect(alias=\"default\", host=\"127.0.0.1\", port=\"19530\")\n",
    "\n",
    "print(\"Milvus connected:\", connections.has_connection(alias=\"default\"))\n",
    "\n",
    "collection_name = \"wiki_movie_plots\"\n",
    "dim = 768  # Dimensions for the vector embeddings\n",
    "\n",
    "# Check if the collection exists and drop it if it does\n",
    "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!   WILL DROP\n",
    "# if utility.has_collection(collection_name):\n",
    "#     collection = Collection(collection_name)\n",
    "#     collection.drop()\n",
    "#     print(f\"Collection '{collection_name}' dropped.\")\n",
    "\n",
    "\n",
    "fields = [\n",
    "    FieldSchema(name=\"id\", dtype=DataType.INT64, is_primary=True),  # Primary key field\n",
    "    FieldSchema(name=\"plot_embedding\", dtype=DataType.FLOAT_VECTOR, dim=dim),\n",
    "    FieldSchema(name=\"plot_text\", dtype=DataType.VARCHAR, max_length=40000),  # To store original plot text\n",
    "    FieldSchema(name=\"release_year\", dtype=DataType.INT64),\n",
    "    FieldSchema(name=\"title\", dtype=DataType.VARCHAR, max_length=255),\n",
    "]\n",
    "\n",
    "# Create the schema and collection\n",
    "schema = CollectionSchema(fields, description=\"Wikipedia Movie Plots with vector embeddings and original plot text\")\n",
    "collection = Collection(name=collection_name, schema=schema)\n",
    "\n",
    "print(\"Milvus collection schema created successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_embeddings(batch_texts, model, tokenizer, device, remove_token_type_ids=False, mean_pooling=True):\n",
    "    \"\"\"\n",
    "    Generalized function to get embeddings for different use cases.\n",
    "    \n",
    "    Parameters:\n",
    "    - batch_texts: List of input texts to be embedded.\n",
    "    - model: The pre-trained model used for generating embeddings.\n",
    "    - tokenizer: The tokenizer corresponding to the model.\n",
    "    - device: The device to run the model on ('cpu', 'cuda', 'mps').\n",
    "    - remove_token_type_ids: Set to True if 'token_type_ids' should be removed from input dict (e.g., for causal LMs like GPT/Mistral).\n",
    "    - mean_pooling: Set to True if you want to perform mean pooling over the output embeddings.\n",
    "    \n",
    "    Returns:\n",
    "    - embeddings: The embeddings for the input batch.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(batch_texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n",
    "    if remove_token_type_ids and 'token_type_ids' in inputs:\n",
    "        del inputs['token_type_ids']\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    if mean_pooling:\n",
    "        return outputs.last_hidden_state.mean(dim=1)\n",
    "    else:\n",
    "        return outputs.logits.mean(dim=1)\n",
    "\n",
    "\n",
    "import re\n",
    "\n",
    "def clean_plot(plot):\n",
    "    plot = re.sub(r'\\[.*?\\]', '', plot)  # Remove anything in square brackets\n",
    "    plot = re.sub(r'\\s+', ' ', plot)     # Replace multiple spaces with a single space\n",
    "    plot = plot.strip()                  # Remove leading/trailing spaces\n",
    "    return plot.lower()                  # Convert to lowercase\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from pymilvus import Collection\n",
    "\n",
    "def search_similar_plots(plot_text, *args, top_k=5, **kwargs):\n",
    "\n",
    "    collection = Collection(\"wiki_movie_plots\")\n",
    "\n",
    "    embedding = get_batch_embeddings([plot_text], *args, **kwargs)\n",
    "    embedding_cpu = embedding.cpu().numpy().squeeze()\n",
    "    embedding_list = embedding_cpu.tolist()\n",
    "    \n",
    "    results = collection.search(\n",
    "        data=[embedding_list],\n",
    "        anns_field=\"plot_embedding\",\n",
    "        param={\"metric_type\": \"L2\"},\n",
    "        limit=top_k,\n",
    "        output_fields=[\"id\"]\n",
    "    )\n",
    "\n",
    "    # wtf\n",
    "    csv_file_path = 'data/wiki_movie_plots_deduped.csv'\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "    df = df[['Release Year', 'Title', 'Plot']].dropna()\n",
    "    \n",
    "    response = []\n",
    "    for result in results[0]:\n",
    "        original_id = result.entity.get(\"id\")\n",
    "        \n",
    "        if original_id < len(df):\n",
    "            plot_text = df.iloc[original_id]['Plot']\n",
    "            title = df.iloc[original_id]['Title']\n",
    "            release_year = df.iloc[original_id]['Release Year']\n",
    "            response.append({\n",
    "                \"title\": title,\n",
    "                \"release_year\": release_year,\n",
    "                \"plot_text\": plot_text,\n",
    "                \"score\": result.distance\n",
    "            })\n",
    "        else:\n",
    "            print(f\"Result ID {original_id} is out of bounds for the DataFrame.\")\n",
    "\n",
    "    return response\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from pymilvus import Collection\n",
    "\n",
    "def insert_embeddings_to_milvus(\n",
    "    csv_file_path, \n",
    "    collection, \n",
    "    embeddings_model_name, \n",
    "    batch_size=32, \n",
    "    device=None\n",
    "):\n",
    "    \n",
    "    df = pd.read_csv(csv_file_path)\n",
    "    df = df[['Release Year', 'Title', 'Plot']].dropna()\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(embeddings_model_name)\n",
    "    model = AutoModel.from_pretrained(embeddings_model_name)\n",
    "\n",
    "    device = device or torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    for i in range(0, len(df), batch_size):\n",
    "        batch_texts = [clean_plot(text) for text in df['Plot'].iloc[i:i+batch_size].tolist()]\n",
    "        batch_titles = df['Title'].iloc[i:i+batch_size].tolist()\n",
    "        batch_release_year = df['Release Year'].iloc[i:i+batch_size].tolist()\n",
    "        batch_ids = df.index[i:i+batch_size].tolist() \n",
    "        batch_embeddings = get_batch_embeddings(batch_texts, model, tokenizer, device, mean_pooling=True)\n",
    "        batch_embeddings_cpu = batch_embeddings.cpu().numpy()\n",
    "\n",
    "        # Prepare records\n",
    "        records = [\n",
    "            {\n",
    "                \"id\": id_value,\n",
    "                \"release_year\": release_year,\n",
    "                \"title\": title,\n",
    "                \"plot_embedding\": embedding.tolist(),  # Convert to list for insertion\n",
    "                \"plot_text\": text\n",
    "            }\n",
    "            for id_value, release_year, title, embedding, text in zip(batch_ids, batch_release_year, batch_titles, batch_embeddings_cpu, batch_texts)\n",
    "        ]\n",
    "        collection.insert(records)\n",
    "\n",
    "    # Flush and create index\n",
    "    collection.flush()\n",
    "    collection.create_index(field_name=\"plot_embedding\", index_params={\"index_type\": \"IVF_FLAT\", \"metric_type\": \"L2\", \"params\": {\"nlist\": 100}})\n",
    "    collection.load()\n",
    "\n",
    "    # Clear memory\n",
    "    torch.mps.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insert data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "from pymilvus import Collection\n",
    "\n",
    "\n",
    "\n",
    "# csv_file_path = 'data/wiki_movie_plots_deduped.csv'\n",
    "# embeddings_model_name = \"sentence-transformers/bert-base-nli-mean-tokens\"\n",
    "# collection = Collection(\"wiki_movie_plots\")\n",
    "\n",
    "# insert_embeddings_to_milvus(csv_file_path, collection, embeddings_model_name)\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doublecheck ingestion row count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records in the collection: 34886\n",
      "Sample records: data: ['{\\'id\\': 1, \\'plot_text\\': \"the moon, painted with a smiling face hangs over a park at night. a young couple walking past a fence learn on a railing and look up. the moon smiles. they embrace, and the moon\\'s smile gets bigger. they then sit down on a bench by a tree. the moon\\'s view is blocked, causing him to frown. in the last scene, the man fans the woman with his hat because the moon has left the sky and is perched over her shoulder to see everything better.\"}', \"{'id': 2, 'plot_text': 'the film, just over a minute long, is composed of two shots. in the first, a girl sits at the base of an altar or tomb, her face hidden from the camera. at the center of the altar, a viewing portal displays the portraits of three u.s. presidents—abraham lincoln, james a. garfield, and william mckinley—each victims of assassination. in the second shot, which runs just over eight seconds long, an assassin kneels feet of lady justice.'}\", '{\\'id\\': 3, \\'plot_text\\': \\'lasting just 61 seconds and consisting of two shots, the first shot is set in a wood during winter. the actor representing then vice-president theodore roosevelt enthusiastically hurries down a hillside towards a tree in the foreground. he falls once, but rights himself and cocks his rifle. two other men, bearing signs reading \"his photographer\" and \"his press agent\" respectively, follow him into the shot; the photographer sets up his camera. \"teddy\" aims his rifle upward at the tree and fells what appears to be a common house cat, which he then proceeds to stab. \"teddy\" holds his prize aloft, and the press agent takes notes. the second shot is taken in a slightly different part of the wood, on a path. \"teddy\" rides the path on his horse towards the camera and out to the left of the shot, followed closely by the press agent and photographer, still dutifully holding their signs.\\'}', '{\\'id\\': 4, \\'plot_text\\': \"the earliest known adaptation of the classic fairytale, this films shows jack trading his cow for the beans, his mother forcing him to drop them in the front yard, and beig forced upstairs. as he sleeps, jack is visited by a fairy who shows him glimpses of what will await him when he ascends the bean stalk. in this version, jack is the son of a deposed king. when jack wakes up, he finds the beanstalk has grown and he climbs to the top where he enters the giant\\'s home. the giant finds jack, who narrowly escapes. the giant chases jack down the bean stalk, but jack is able to cut it down before the giant can get to safety. he falls and is killed as jack celebrates. the fairy then reveals that jack may return home as a prince.\"}', '{\\'id\\': 5, \\'plot_text\\': \\'alice follows a large white rabbit down a \"rabbit-hole\". she finds a tiny door. when she finds a bottle labeled \"drink me\", she does, and shrinks, but not enough to pass through the door. she then eats something labeled \"eat me\" and grows larger. she finds a fan when enables her to shrink enough to get into the \"garden\" and try to get a \"dog\" to play with her. she enters the \"white rabbit\\\\\\'s tiny house,\" but suddenly resumes her normal size. in order to get out, she has to use the \"magic fan.\" she enters a kitchen, in which there is a cook and a woman holding a baby. she persuades the woman to give her the child and takes the infant outside after the cook starts throwing things around. the baby then turns into a pig and squirms out of her grip. \"the duchess\\\\\\'s cheshire cat\" appears and disappears a couple of times to alice and directs her to the mad hatter\\\\\\'s \"mad tea-party.\" after a while, she leaves. the queen invites alice to join the \"royal procession\": a parade of marching playing cards and others headed by the white rabbit. when alice \"unintentionally offends the queen\", the latter summons the \"executioner\". alice \"boxes the ears\", then flees when all the playing cards come for her. then she wakes up and realizes it was all a dream.\\'}'] \n"
     ]
    }
   ],
   "source": [
    "## Run this to connect to db if already performed data insert\n",
    "## ie: you run out or memory and restart the kernel and need to reconnect to db\n",
    "\n",
    "from pymilvus import connections, Collection\n",
    "\n",
    "# Connect to Milvus\n",
    "connections.connect(alias=\"default\", host=\"localhost\", port=\"19530\")\n",
    "collection = Collection(\"wiki_movie_plots\")\n",
    "collection.load()\n",
    "\n",
    "num_records = collection.num_entities\n",
    "print(f\"Number of records in the collection: {num_records}\")\n",
    "\n",
    "if num_records > 0:\n",
    "    sample_records = collection.query(expr=\"id >= 1\", output_fields=[\"id\", \"plot_text\"], limit=5)\n",
    "    print(\"Sample records:\", sample_records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:18<00:00,  6.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "got loaded\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModel\n",
    "import os, torch\n",
    "\n",
    "home_dir = os.getenv(\"HOME\")\n",
    "\n",
    "rag_model_name = f'{home_dir}/ext-gits/Mistral-7B-Instruct-v0.3'\n",
    "rag_tokenizer = AutoTokenizer.from_pretrained(rag_model_name)\n",
    "rag_device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "rag_model = AutoModelForCausalLM.from_pretrained(\n",
    "    rag_model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map={\"\": device},\n",
    "    trust_remote_code=True\n",
    "    )\n",
    "\n",
    "if rag_tokenizer.pad_token is None:\n",
    "    rag_tokenizer.add_special_tokens({'pad_token':rag_tokenizer.eos_token})\n",
    "\n",
    "rag_model.to(device)\n",
    "\n",
    "\n",
    "db_model_name = \"sentence-transformers/bert-base-nli-mean-tokens\"\n",
    "db_tokenizer = AutoTokenizer.from_pretrained(db_model_name)\n",
    "db_model = AutoModel.from_pretrained(db_model_name)\n",
    "\n",
    "db_device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "db_model.to(device)\n",
    "\n",
    "print('got loaded')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "outcome:\n",
      "\n",
      "\n",
      "Givent the following descriptions a flight turns dangerous when the passengers find a ton of snakes on boardA deranged man calling himself James Pettis (Val Kilmer) approaches The Grand Rapids Press demanding that it publish his predictions about the upcoming demise of civilization due to the conditions of global warming, warning that he has trapped a group of six people in a Turkish-style steamroom to demonstrate the effects of this environment on humans.[1] A local police detective Mancini (Armand Assante) tries to get Pettis to reveal information that will help him confirm the truth of his threat and to rescue the hostages, but over the course of the interrogation begins to suspect that either Pettis' story is a delusional hoax, or that the steamroom killing has already taken place.\n",
      "As Pettis describes developments in the streamroom to Mancini, the scene is shown of three men and three women meeting in the steamroom of a luxury hotel as part of an online dating promotion, then being locked in together. When they discover that they have been locked in, they react badly: Frank (Quinn Duffy) becomes abusive to Jessie (Eve Mauro), and is killed in her defense by openly neurotic Margaret (Cordelia Reynolds). Jessie is killed with a nail gun by an unseen assailant when she pokes her head through the small window in the steamroom door; Christopher (Patrick Muldoon) is injured in the hand with a nail as the window is boarded over from outside. Margaret becomes agitated and commits suicide. Grant (Eric Roberts) is bludgeoned by Catherine (Megan Brown) after he accuses her and Christopher of being allies of the perpetrators and repeatedly holds her head underwater.\n",
      "Mancini's call to Pettis' psychiatrist finally brings staff from the local state psychiatric hospital, from which Pettis recently escaped. It is revealed that Christopher and Catherine are staff at this facility, and are unhappy with Pettis for going to the news media and police with this story.\n",
      "\n",
      "A college student Seema sees in her nightmare that a horrible looking man wearing steel claw gloves attacks her. She wakes up to find that she has real wounds on her arm. Later her friend Anita too sees the same nightmare and finds real wounds on her arm. Anita tells her parents about her nightmare. Her father, who is a policeman, refuses to believe it. Later, Seema is attacked again in her dream and she dies because of the wounds. Her boyfriend is put in police lockup where he sees the horrible man who makes snakes appear by magic. The boyfriend dies of snake-bite. Later, Anita and her mother catch Anita's father taking out a metal clawed glove from his drawer, so he is forced to reveal the secret that an evil magician Shakal had been kidnapping children and sacrificing them to increase his evil powers. Seven years back, Shakal had killed Anita's sister too. Finally, Shakal kidnaps Anita. As her father knows Shakal's place, he arrives with Anita's boyfriend and they manage to kill him and save Anita., create a new plotline for a movie/tv episode called \"The Steamroom Killer\" or \"The Hostages\" that combines elements of psychological thriller, horror, and suspense genres.\n",
      "\n",
      "In this plotline, a former psychiatrist named James\n"
     ]
    }
   ],
   "source": [
    "def plots_query(query, *args, **kwargs):\n",
    "    search_results = search_similar_plots(query, top_k=2, *args, **kwargs)\n",
    "    context_texts = [res['plot_text'] for res in search_results]\n",
    "    return query + \"\\n\\n\".join(context_texts)\n",
    "\n",
    "def generate_response(combined_texts, *args, max_new_tokens=50):\n",
    "    model, tokenizer, device = args\n",
    "\n",
    "    new_prompt = f'Givent the following descriptions {combined_texts}, create a new plotline'\n",
    "\n",
    "    inputs = tokenizer(new_prompt, return_tensors=\"pt\", padding=True).to(device)\n",
    "\n",
    "    print('')\n",
    "    print('')\n",
    "    print('outcome:')\n",
    "    print('')\n",
    "    print('')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            # max_length=500,\n",
    "            num_return_sequences=1,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            top_p=0.95,\n",
    "            top_k=50,\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "\n",
    "combined_query = plots_query(\n",
    "    \"a flight turns dangerous when the passengers find a ton of snakes on board\",\n",
    "    db_model, \n",
    "    db_tokenizer, \n",
    "    db_device, \n",
    "    mean_pooling=True,\n",
    "    remove_token_type_ids=True\n",
    "    )\n",
    "\n",
    "outcome = generate_response(\n",
    "    combined_query,\n",
    "    rag_model, \n",
    "    rag_tokenizer, \n",
    "    rag_device, \n",
    "    )\n",
    "\n",
    "print(outcome)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODOS:\n",
    "\n",
    "[] Refactor rag implementation\n",
    "\n",
    "## \n",
    "\n",
    "[] Adjust mistral params\n",
    "\n",
    "##\n",
    "\n",
    "[] Create HyDE\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ragtime-5o5M5Lb8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
