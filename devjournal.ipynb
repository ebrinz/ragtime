{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG / HyDE Example with Mistral Instruct 7b and Milvus DB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports dependencies and Mistral LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/crashy/.local/share/virtualenvs/ragtime-5o5M5Lb8/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "8-bit quantization failed: Using `bitsandbytes` 8-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`\n",
      "Falling back to Float16 precision.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:10<00:00,  3.60s/it]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import torchaudio\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "home_dir = os.getenv(\"HOME\")\n",
    "\n",
    "model_name = f'{home_dir}/ext-gits/Mistral-7B-Instruct-v0.3'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "try:\n",
    "    # Attempt to use 8-bit quantization\n",
    "    quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=quantization_config,\n",
    "        device_map={\"\": device},\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    print(\"Model loaded with 8-bit quantization.\")\n",
    "except Exception as e:\n",
    "    print(f\"8-bit quantization failed: {e}\")\n",
    "    print(\"Falling back to Float16 precision.\")\n",
    "    # Load the model with Float16 precision\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map={\"\": device},\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, there was a man who was a gardener. He had the most beautiful garden in the whole country, and every day he would get up and go out into it and tend to it, watching it grow and flourish\n"
     ]
    }
   ],
   "source": [
    "\n",
    "prompt = \"Once upon a time\"\n",
    "\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True).to(device)\n",
    "\n",
    "# Generate response\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        max_length=50,\n",
    "        num_return_sequences=1,\n",
    "        do_sample=True,\n",
    "        top_p=0.95,\n",
    "        top_k=50,\n",
    "        pad_token_id=tokenizer.pad_token_id\n",
    "    )\n",
    "\n",
    "# Decode and print the response\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Up Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The longest plot is at index 26064 with length 36773 characters.\n",
      "Longest Plot: After a brief introduction to some of the main characters of the story, the beginning sees a group of Rishis, led by Vishvamitra, performing a Yajna in a forest not far from Ayodhya, the Capital of the Kingdom of Kosala. This Yajna, like several before it, is interrupted and destroyed by a group of flying demons led by Ravana's Mama(Uncle/Mother's Brother) Maricha. After seeing yet another Yajna destroyed, a despondent Vishvamitra appeals to Lord Vishnu for salvation. Vishnu appears in a spiritu...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('data/wiki_movie_plots_deduped.csv')\n",
    "\n",
    "# Ensure there are no NaNs in the 'Plot' column\n",
    "df = df[['Plot']].dropna()\n",
    "\n",
    "# Find the longest plot and its length\n",
    "longest_plot = df['Plot'].apply(len).idxmax()  # Find the index of the longest plot\n",
    "longest_plot_text = df['Plot'].iloc[longest_plot]  # Get the longest plot text\n",
    "longest_plot_length = len(longest_plot_text)  # Get the length of the longest plot text\n",
    "\n",
    "print(f\"The longest plot is at index {longest_plot} with length {longest_plot_length} characters.\")\n",
    "print(f\"Longest Plot: {longest_plot_text[:500]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Milvus connected: True\n",
      "Collection 'wiki_movie_plots' dropped.\n",
      "Milvus collection schema created successfully!\n"
     ]
    }
   ],
   "source": [
    "# run milvis locally\n",
    "# run: docker-compose up -d\n",
    "\n",
    "# data is ..data/wiki_movie_plots_deduped.csv is from\n",
    "# https://www.kaggle.com/datasets/jrobischon/wikipedia-movie-plots\n",
    "\n",
    "from pymilvus import connections, CollectionSchema, DataType, FieldSchema, Collection, utility\n",
    "\n",
    "# Connect to Milvus\n",
    "connections.connect(alias=\"default\", host=\"127.0.0.1\", port=\"19530\")\n",
    "\n",
    "# Check Milvus status\n",
    "print(\"Milvus connected:\", connections.has_connection(alias=\"default\"))\n",
    "\n",
    "# Define the schema for the collection\n",
    "collection_name = \"wiki_movie_plots\"\n",
    "dim = 768  # Dimensions for the vector embeddings\n",
    "\n",
    "# Check if the collection exists and drop it if it does\n",
    "# !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!   WILL DROP\n",
    "# if utility.has_collection(collection_name):\n",
    "#     collection = Collection(collection_name)\n",
    "#     collection.drop()\n",
    "#     print(f\"Collection '{collection_name}' dropped.\")\n",
    "\n",
    "\n",
    "fields = [\n",
    "    FieldSchema(name=\"id\", dtype=DataType.INT64, is_primary=True, auto_id=True),  # Primary key field\n",
    "    FieldSchema(name=\"plot_embedding\", dtype=DataType.FLOAT_VECTOR, dim=dim),\n",
    "    FieldSchema(name=\"plot_text\", dtype=DataType.VARCHAR, max_length=40000),  # To store original plot text\n",
    "    FieldSchema(name=\"release_year\", dtype=DataType.INT64),\n",
    "    FieldSchema(name=\"title\", dtype=DataType.VARCHAR, max_length=255),\n",
    "]\n",
    "\n",
    "# Create the schema and collection\n",
    "schema = CollectionSchema(fields, description=\"Wikipedia Movie Plots with vector embeddings and original plot text\")\n",
    "collection = Collection(name=collection_name, schema=schema)\n",
    "\n",
    "print(\"Milvus collection schema created successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records in the collection: 34886\n",
      "Sample records: data: ['{\\'id\\': 453300591135312087, \\'plot_text\\': \"a bartender is working at a saloon, serving drinks to customers. after he fills a stereotypically irish man\\'s bucket with beer, carrie nation and her followers burst inside. they assault the irish man, pulling his hat over his eyes and then dumping the beer over his head. the group then begin wrecking the bar, smashing the fixtures, mirrors, and breaking the cash register. the bartender then sprays seltzer water in nation\\'s face before a group of policemen appear and order everybody to leave.\"}', '{\\'id\\': 453300591135312088, \\'plot_text\\': \"the moon, painted with a smiling face hangs over a park at night. a young couple walking past a fence learn on a railing and look up. the moon smiles. they embrace, and the moon\\'s smile gets bigger. they then sit down on a bench by a tree. the moon\\'s view is blocked, causing him to frown. in the last scene, the man fans the woman with his hat because the moon has left the sky and is perched over her shoulder to see everything better.\"}', \"{'id': 453300591135312089, 'plot_text': 'the film, just over a minute long, is composed of two shots. in the first, a girl sits at the base of an altar or tomb, her face hidden from the camera. at the center of the altar, a viewing portal displays the portraits of three u.s. presidents—abraham lincoln, james a. garfield, and william mckinley—each victims of assassination. in the second shot, which runs just over eight seconds long, an assassin kneels feet of lady justice.'}\", '{\\'id\\': 453300591135312090, \\'plot_text\\': \\'lasting just 61 seconds and consisting of two shots, the first shot is set in a wood during winter. the actor representing then vice-president theodore roosevelt enthusiastically hurries down a hillside towards a tree in the foreground. he falls once, but rights himself and cocks his rifle. two other men, bearing signs reading \"his photographer\" and \"his press agent\" respectively, follow him into the shot; the photographer sets up his camera. \"teddy\" aims his rifle upward at the tree and fells what appears to be a common house cat, which he then proceeds to stab. \"teddy\" holds his prize aloft, and the press agent takes notes. the second shot is taken in a slightly different part of the wood, on a path. \"teddy\" rides the path on his horse towards the camera and out to the left of the shot, followed closely by the press agent and photographer, still dutifully holding their signs.\\'}', '{\\'id\\': 453300591135312091, \\'plot_text\\': \"the earliest known adaptation of the classic fairytale, this films shows jack trading his cow for the beans, his mother forcing him to drop them in the front yard, and beig forced upstairs. as he sleeps, jack is visited by a fairy who shows him glimpses of what will await him when he ascends the bean stalk. in this version, jack is the son of a deposed king. when jack wakes up, he finds the beanstalk has grown and he climbs to the top where he enters the giant\\'s home. the giant finds jack, who narrowly escapes. the giant chases jack down the bean stalk, but jack is able to cut it down before the giant can get to safety. he falls and is killed as jack celebrates. the fairy then reveals that jack may return home as a prince.\"}'] \n"
     ]
    }
   ],
   "source": [
    "# doublecheck\n",
    "\n",
    "from pymilvus import connections, Collection\n",
    "\n",
    "# Connect to Milvus\n",
    "# connections.connect(alias=\"default\", host=\"localhost\", port=\"19530\")\n",
    "collection.load()\n",
    "\n",
    "# Load the collection\n",
    "collection = Collection(\"wiki_movie_plots\")\n",
    "\n",
    "# Check how many records are in the collection\n",
    "num_records = collection.num_entities\n",
    "print(f\"Number of records in the collection: {num_records}\")\n",
    "\n",
    "# Query a few records to inspect\n",
    "if num_records > 0:\n",
    "    sample_records = collection.query(expr=\"id >= 1\", output_fields=[\"id\", \"plot_text\"], limit=5)\n",
    "    print(\"Sample records:\", sample_records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insert data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plot Embedding: tensor([[ 7.2634e-02,  1.7146e-01, -5.8976e-04, -4.3674e-02, -9.8264e-02,\n",
      "          8.6167e-02, -1.2725e-01, -9.4895e-02, -2.9519e-02,  8.2841e-02,\n",
      "          4.7391e-02, -3.6577e-02,  5.8282e-02,  8.1094e-02, -1.6779e-02,\n",
      "          1.1979e-01,  6.7246e-02,  7.4323e-02,  4.7637e-02, -7.6078e-02,\n",
      "         -5.8983e-02,  3.1512e-02, -4.1934e-02, -7.7589e-02, -2.4669e-02,\n",
      "          5.4758e-02, -3.5519e-03,  5.1198e-02, -1.4199e-01,  7.2104e-02,\n",
      "          2.6151e-03, -1.6058e-02, -5.1134e-02, -4.6279e-03,  4.9275e-06,\n",
      "         -1.3567e-02, -6.6775e-02, -8.0051e-02, -3.8100e-02, -1.2830e-01,\n",
      "         -1.4797e-01,  6.6426e-02, -2.5168e-02,  5.1333e-02,  5.7209e-02,\n",
      "          1.7733e-02, -1.2465e-02,  1.4359e-01,  3.3481e-02,  2.0943e-02,\n",
      "          1.9472e-02, -3.1945e-02, -1.8737e-01, -1.0638e-01,  1.4778e-02,\n",
      "         -6.3040e-03,  6.7245e-02,  1.5119e-01, -1.4442e-02, -2.4544e-01,\n",
      "         -3.1666e-02,  8.5816e-02, -8.8085e-02,  6.1015e-02, -1.5713e-01,\n",
      "          3.8713e-03, -1.3084e-01,  2.1808e-02,  6.8381e-02, -1.1272e-01,\n",
      "          8.9329e-02, -1.5466e-02,  1.0141e-02,  8.3844e-02, -1.6579e-02,\n",
      "          3.5273e-02, -4.4865e-02, -4.7994e-02, -8.3498e-03, -7.1987e-02,\n",
      "         -1.3605e-01,  1.2302e-01, -1.3151e-02,  2.1585e-03,  4.0482e-02,\n",
      "          2.1838e-01, -6.2196e-02,  4.1819e-02, -3.6700e-03,  1.1848e-02,\n",
      "          4.0107e-02,  1.1616e-01,  6.8292e-02,  1.1971e-01, -1.0569e-01,\n",
      "          5.7680e-03,  4.3172e-02,  1.7499e-01, -2.5108e-02,  6.0168e-02,\n",
      "         -4.0124e-02,  9.1207e-02, -4.6177e-02,  7.5209e-02, -6.3422e-02,\n",
      "         -3.1418e-02,  1.5237e-01, -1.5275e-01,  1.1484e-02, -1.5427e-01,\n",
      "         -8.4882e-02,  4.2894e-02, -5.4711e-02, -6.6077e-02,  5.5054e-02,\n",
      "         -2.3412e-02, -2.8570e-02, -8.4717e-03, -2.0456e-02,  4.0482e-02,\n",
      "          8.0407e-06,  1.0806e-01, -7.5581e-02,  9.5022e-02,  3.1075e-02,\n",
      "          2.8970e-02, -4.6900e-02,  6.9297e-02, -6.3705e-02, -8.7311e-02,\n",
      "         -6.8237e-02,  7.7280e-02,  3.9044e-02,  8.6916e-02, -3.9477e-02,\n",
      "          1.8516e-02, -6.4371e-02, -1.8749e-03, -1.5175e-01,  3.6295e-02,\n",
      "         -1.0870e-02, -1.4517e-01, -1.9072e-01, -1.4813e-01,  1.8921e-01,\n",
      "          3.6733e-02,  2.4089e-02, -1.4884e-01,  9.5167e-02, -5.0822e-03,\n",
      "          8.2474e-02, -8.7214e-02,  4.6704e-03,  1.2783e-01, -7.1794e-04,\n",
      "         -4.5989e-02, -2.2582e-01,  1.2705e-01,  4.5092e-03,  7.1851e-03,\n",
      "         -1.9021e-04, -2.3538e-02, -5.0875e-02, -5.8620e-02, -8.9115e-02,\n",
      "          1.1645e-01, -6.4287e-03,  5.6786e-02, -1.6887e-02,  4.9912e-02,\n",
      "          2.5174e-02, -8.8735e-02,  3.7644e-02,  1.7260e-02,  3.1278e-04,\n",
      "          1.2751e-01, -6.8528e-02,  9.3854e-03,  1.0804e-01,  1.8572e-02,\n",
      "         -1.0403e-01,  4.0402e-02,  5.1918e-02, -2.5446e-01,  4.2097e-02,\n",
      "         -7.7385e-02,  8.3220e-02,  1.8510e-02,  1.9408e-02,  5.2532e-03,\n",
      "          8.1153e-02,  1.2637e-01, -9.3459e-02,  4.1934e-02, -7.3177e-02,\n",
      "          4.9728e-02, -7.7357e-03, -2.1087e-01,  9.0103e-02, -1.1018e-02,\n",
      "          1.9089e-02,  1.7504e-02,  5.8048e-03,  1.7628e-01, -1.6937e-02,\n",
      "          4.8626e-02,  8.2632e-02, -8.4762e-02,  1.3727e-01,  1.1254e-01,\n",
      "          1.2767e-01,  1.7695e-02,  2.7726e-02,  1.6490e-01, -4.1055e-02,\n",
      "          1.2650e-01,  1.4205e-02,  4.1173e-02, -3.2321e-02,  1.6144e-01,\n",
      "          2.0254e-02,  1.9386e-01, -1.6680e-01, -7.6140e-02,  5.3019e-02,\n",
      "         -4.4915e-02, -5.7448e-02, -4.6976e-02,  5.2629e-02,  1.5192e-02,\n",
      "         -1.0859e-01, -7.9971e-02,  5.1675e-02, -1.3091e-01, -1.1444e-03,\n",
      "          1.4841e-01,  4.1841e-02,  2.4759e-02,  3.2911e-02,  7.7827e-02,\n",
      "         -1.0083e-01, -9.9520e-02,  3.4790e-02, -1.2183e-02,  1.6599e-01,\n",
      "          5.8060e-02, -1.0923e-02, -7.4349e-02, -6.4518e-02,  9.0747e-02,\n",
      "          8.9896e-03, -2.3674e-02,  3.8050e-02, -4.1562e-03,  8.9273e-02,\n",
      "         -1.5440e-01, -9.7448e-02, -3.6993e-02,  3.8805e-02,  9.1937e-02,\n",
      "         -1.1850e-02, -4.0729e-02,  8.6960e-02, -5.2645e-02, -1.1360e-01,\n",
      "          1.0979e-01,  1.1421e-01,  5.5937e-02,  6.7452e-02, -7.2765e-02,\n",
      "         -6.9840e-02, -8.5484e-02,  1.1200e-01, -1.0598e-01,  1.7204e-02,\n",
      "         -6.8301e-02,  1.8255e-04,  6.6810e-02, -3.9551e-02, -2.9988e-02,\n",
      "          9.8457e-02, -8.4709e-02,  4.5217e-02,  1.1285e-01, -6.9488e-02,\n",
      "          5.3441e-02, -2.1210e-02, -7.8113e-02, -8.1198e-02, -1.5894e-01,\n",
      "          1.2561e-01, -4.6329e-02,  6.8485e-02,  3.4464e-02, -2.8088e-02,\n",
      "          5.4448e-02,  2.0111e-01, -1.2501e-01, -1.1454e-01, -6.6304e-02,\n",
      "         -4.4002e-02, -1.0312e-02,  1.5298e-03, -1.4997e-02,  3.0398e-02,\n",
      "         -1.0939e-01, -5.1276e-02, -9.0744e-02,  3.3276e-02, -8.8477e-02,\n",
      "          4.3722e-02,  9.8216e-02, -1.6464e-03, -5.7667e-03, -3.9789e-02,\n",
      "         -3.9083e-02, -2.3622e-01,  4.7017e-02, -1.3772e-01,  1.2445e-02,\n",
      "          5.1421e-02,  1.1252e-01, -5.4872e-02, -1.6251e-02,  3.9584e-02,\n",
      "          1.5446e-02, -8.1526e-02, -1.0505e-01,  2.2306e-02,  6.2017e-02,\n",
      "          1.1436e-01, -9.9539e-02, -2.0662e-02, -6.5209e-02,  8.8155e-02,\n",
      "         -8.3783e-02, -8.9366e-03,  1.2279e-01, -1.6146e-02, -3.6160e-02,\n",
      "         -5.7967e-03,  3.7415e-02, -8.0600e-03, -9.8137e-02, -9.9932e-02,\n",
      "          1.5135e-02,  5.8809e-02,  1.0576e-01, -2.8350e-02,  1.1157e-01,\n",
      "          9.5986e-02,  4.4616e-02, -5.8229e-02,  1.0800e-02, -5.9751e-02,\n",
      "          1.1998e-01, -3.5374e-02,  1.7824e-01, -4.1490e-02,  8.9731e-02,\n",
      "         -6.8992e-02, -6.4273e-02, -5.4174e-02,  1.9609e-02, -5.0666e-02,\n",
      "          1.0896e-01,  5.3103e-02, -4.0426e-02,  1.5003e-01,  1.4402e-02,\n",
      "          2.6642e-02, -7.2559e-03,  5.0227e-02,  3.8521e-02,  4.8191e-02,\n",
      "          1.3022e-01, -4.9437e-02, -5.5208e-02, -5.4609e-02,  4.3855e-02,\n",
      "          1.8152e-01, -2.0425e-01, -5.3727e-03,  1.5223e-01,  1.0962e-02,\n",
      "          5.2796e-02, -9.2086e-02, -2.1697e-01,  1.6592e-02,  1.3120e-01,\n",
      "          3.2747e-02, -4.5088e-02, -1.7237e-01, -4.8182e-02,  8.9341e-02,\n",
      "         -1.3943e-01, -1.1086e-01,  7.5667e-02,  8.7386e-03, -4.4648e-02,\n",
      "          7.8373e-02,  2.0741e-01,  5.0776e-02,  1.4502e-03, -1.0901e-01,\n",
      "         -2.1569e-02,  8.4438e-02, -1.8189e-02,  1.0495e-01,  2.5038e-01,\n",
      "         -2.0266e-01, -2.4522e-01,  1.0083e-01,  8.5389e-02, -6.7630e-02,\n",
      "         -7.4734e-02, -6.0566e-02, -8.0383e-02, -1.3875e-02, -3.2365e-02,\n",
      "          5.1257e-02, -1.1366e-02, -9.4319e-02, -3.4626e-02, -3.3078e-02,\n",
      "         -4.1568e-02,  8.8369e-02,  8.1707e-02,  1.6933e-01,  1.9858e-01,\n",
      "          4.7169e-02,  3.8428e-02, -1.1586e-01, -1.0027e-01, -1.5776e-01,\n",
      "         -2.7294e-02, -4.2679e-02,  1.1088e-01, -1.8239e-01,  2.2227e-02,\n",
      "          5.1083e-02,  3.3306e-02, -8.5995e-02, -2.5450e-02, -1.6598e-02,\n",
      "         -1.9389e-01, -1.8233e-02,  4.9495e-02, -4.2399e-02, -2.2646e-01,\n",
      "          5.4649e-02, -6.0752e-03, -1.6850e-01,  1.9074e-01, -3.8472e-02,\n",
      "          5.0356e-03,  5.9052e-02, -8.2273e-02, -3.2475e-02,  2.6975e-01,\n",
      "          4.4449e-02,  1.7242e-02,  9.7780e-02,  1.7565e-02,  3.4174e-02,\n",
      "          8.5952e-02,  6.3674e-02, -2.0752e-01,  7.1278e-02, -8.8518e-02,\n",
      "          1.5780e-01, -6.8718e-02,  8.7079e-03, -1.4181e-01,  7.9996e-02,\n",
      "         -1.0486e-02, -3.2487e-02, -1.9844e-01,  1.2327e-01,  2.7708e-02,\n",
      "          2.5333e-02, -6.8637e-02, -4.0206e-05, -1.0523e-01,  7.0750e-02,\n",
      "          5.0902e-02,  5.0237e-02, -8.5116e-02,  6.6855e-02, -1.2173e-01,\n",
      "         -1.5543e-02, -3.1124e-02, -1.7038e-02,  3.9911e-03, -3.0180e-02,\n",
      "          7.8509e-02, -3.2674e-02, -3.9718e-02,  2.9740e-01, -1.9051e-01,\n",
      "         -6.8414e-02,  2.1472e-02,  1.0606e-02,  2.3388e-01, -4.5356e-02,\n",
      "         -1.2680e-02, -3.1699e-03, -7.1809e-02, -5.6577e-02, -7.0355e-02,\n",
      "         -2.9973e-03,  1.0925e-01, -1.6881e-01, -1.8706e-01,  4.3005e-02,\n",
      "         -8.1218e-03, -9.5769e-02, -1.6051e-01, -4.7039e-02,  3.0092e-02,\n",
      "          1.6973e-02, -5.7651e-04, -3.1721e-03, -9.5901e-02,  1.0684e-01,\n",
      "         -5.7866e-02,  5.8667e-02,  7.9483e-02, -1.0879e-01,  5.0668e-02,\n",
      "         -1.3009e-01, -3.2468e-03,  3.1574e-01,  1.0407e-01, -7.2082e-02,\n",
      "          2.6261e-02,  7.2616e-02, -3.9104e-02,  3.3334e-02, -8.7170e-03,\n",
      "         -3.2635e-03,  1.3974e-01, -3.2907e-02,  2.5823e-02,  9.0718e-02,\n",
      "          9.3890e-02, -1.2028e-01,  3.7576e-02, -3.9429e-02,  1.6871e-02,\n",
      "         -1.3254e-01, -5.2955e-02,  8.6865e-02, -5.8154e-02,  8.7548e-02,\n",
      "         -1.6635e-32, -1.3271e-01, -5.0414e-02,  1.5301e-02,  1.1613e-01,\n",
      "         -1.0865e-02, -1.2295e-01, -4.8152e-02,  1.0986e-02,  6.4802e-03,\n",
      "          8.8826e-03, -3.2555e-02,  4.6056e-02, -5.0310e-04, -2.9536e-02,\n",
      "          1.2152e-01, -6.3367e-02,  4.2689e-02,  1.1932e-02,  8.6064e-02,\n",
      "          6.1532e-02,  3.1422e-02,  5.4964e-02, -1.1630e-01, -1.9944e-01,\n",
      "         -7.7906e-02,  9.5253e-02, -7.2201e-02,  2.8532e-02,  1.2754e-01,\n",
      "         -1.2052e-01,  2.4936e-02,  2.4220e-01,  1.8587e-02, -6.8373e-02,\n",
      "          8.1305e-02, -3.2684e-02, -6.8561e-02, -1.1715e-01, -7.1492e-02,\n",
      "          1.0759e-01, -2.8314e-03, -6.0532e-02,  8.4241e-02, -7.7956e-02,\n",
      "          1.1135e-01,  4.8927e-02,  2.6325e-02,  6.7175e-02,  5.3389e-02,\n",
      "         -1.5707e-01,  2.2100e-02,  1.0700e-02, -5.3625e-02, -1.5564e-01,\n",
      "          1.5794e-02,  3.8839e-02,  2.3744e-02, -8.7409e-02, -5.6493e-02,\n",
      "         -7.7989e-02, -8.0144e-02, -2.7788e-02,  6.2662e-02, -1.2428e-01,\n",
      "          1.6907e-03,  3.6319e-02,  8.8079e-02, -2.4882e-02, -1.7492e-02,\n",
      "         -2.7638e-02, -8.2503e-02, -1.4756e-02, -9.7836e-03, -1.5266e-01,\n",
      "          1.0720e-01, -8.0450e-02, -2.8603e-03,  3.6977e-02,  1.7125e-01,\n",
      "         -1.4467e-02, -6.3537e-02, -6.4863e-02, -4.9539e-02, -3.7075e-03,\n",
      "          7.1778e-02, -1.7107e-01, -2.0733e-03, -5.1298e-02, -9.9920e-02,\n",
      "          3.2234e-02,  8.1972e-03,  1.2533e-01,  4.0267e-02, -2.7472e-03,\n",
      "         -2.6559e-02,  5.3141e-02,  1.0358e-01, -5.3596e-02,  6.1267e-02,\n",
      "          6.1267e-02, -4.8655e-02,  8.1099e-02, -1.3670e-01, -9.4977e-03,\n",
      "          9.6081e-02,  3.2711e-02,  7.0057e-02,  9.7956e-02, -1.7197e-01,\n",
      "          3.7665e-02,  8.9174e-02,  1.0350e-01, -7.8552e-03,  1.4035e-01,\n",
      "          3.9007e-02,  2.5815e-02,  2.1679e-02, -9.2683e-02,  8.7298e-02,\n",
      "         -1.3518e-01,  2.2121e-01,  2.3076e-02, -4.2028e-02,  8.9473e-02,\n",
      "         -1.0790e-01,  8.4830e-03,  6.1687e-02, -5.2501e-02,  8.9469e-02,\n",
      "          6.8172e-02, -4.2774e-02, -1.0158e-01,  6.9678e-07,  8.9247e-02,\n",
      "         -8.4236e-02, -2.8349e-02,  9.9170e-02, -5.9617e-02, -1.8016e-01,\n",
      "          3.3837e-02,  6.4393e-02,  3.2474e-03, -1.7833e-01,  2.6693e-02,\n",
      "         -1.8460e-02,  3.5505e-02,  9.7499e-02, -1.2184e-01, -7.9156e-03,\n",
      "         -2.9712e-02, -9.3930e-02,  9.1735e-02, -1.1479e-01, -4.4648e-03,\n",
      "         -4.1960e-02,  6.9482e-03,  2.4512e-02,  9.9116e-03, -1.9333e-01,\n",
      "         -7.0251e-02, -1.7065e-01,  3.7912e-02,  6.0991e-02, -2.0066e-01,\n",
      "          1.3419e-01,  1.7473e-03, -3.5776e-02,  5.8825e-02,  8.1126e-02,\n",
      "          3.4373e-02,  2.4509e-02, -7.7073e-02,  1.6527e-01, -4.6508e-02,\n",
      "          9.5965e-02, -4.3796e-02,  2.1583e-01,  7.7235e-03, -1.3571e-01,\n",
      "          2.4132e-02,  7.1114e-02, -3.2174e-02, -6.4209e-03,  8.9050e-02,\n",
      "          2.8795e-02,  2.7129e-02,  7.1104e-03,  8.1939e-03,  4.5352e-02,\n",
      "          2.7380e-02,  2.0248e-02, -2.2564e-02, -9.6384e-02,  5.1913e-02,\n",
      "          1.2793e-01,  5.3744e-02, -1.9965e-01, -9.1492e-02, -1.8526e-01,\n",
      "         -4.8471e-02,  5.3845e-34,  1.1826e-01, -6.0916e-02,  9.3050e-02,\n",
      "          1.4460e-01, -1.2918e-02, -3.8959e-02, -7.9483e-02, -3.5909e-02,\n",
      "          1.8632e-02,  1.6201e-01, -5.1826e-02]], device='mps:0')\n",
      "Inserted 34886 records into Milvus.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('data/wiki_movie_plots_deduped.csv')\n",
    "\n",
    "# Filter the columns we are interested in\n",
    "df = df[['Release Year', 'Title', 'Plot']].dropna()\n",
    "\n",
    "# Load the pre-trained transformer model for embeddings\n",
    "model_name = \"sentence-transformers/all-MPNet-base-v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# Move model to the device (CPU or MPS for MacBooks)\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Function to clean movie plots\n",
    "def clean_plot(plot):\n",
    "    plot = re.sub(r'\\[.*?\\]', '', plot)  # Remove anything in square brackets\n",
    "    plot = re.sub(r'\\s+', ' ', plot)     # Replace multiple spaces with a single space\n",
    "    plot = plot.strip()                  # Remove leading/trailing spaces\n",
    "    return plot.lower()                  # Convert to lowercase\n",
    "\n",
    "# Function to get vector embeddings from a plot\n",
    "def get_batch_embeddings(batch_texts):\n",
    "    inputs = tokenizer(batch_texts, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1)  # Get sentence embedding\n",
    "\n",
    "batch_size = 32  # Adjust according to your memory capacity\n",
    "\n",
    "for i in range(0, len(df), batch_size):\n",
    "    batch_texts = [clean_plot(text) for text in df['Plot'].iloc[i:i+batch_size].tolist()]\n",
    "    batch_titles = df['Title'].iloc[i:i+batch_size].tolist()\n",
    "    batch_release_year = df['Release Year'].iloc[i:i+batch_size].tolist()\n",
    "\n",
    "    # Get embeddings\n",
    "    batch_embeddings = get_batch_embeddings(batch_texts)\n",
    "    \n",
    "    # Move embeddings to CPU before pushing to Milvus\n",
    "    batch_embeddings_cpu = batch_embeddings.cpu().numpy()\n",
    "\n",
    "    # Prepare records to insert into Milvus\n",
    "    records = [\n",
    "        {\n",
    "            \"release_year\": release_year,\n",
    "            \"title\": title,\n",
    "            \"plot_embedding\": embedding.tolist(),  # Convert to list for insertion\n",
    "            \"plot_text\": text\n",
    "        }\n",
    "        for release_year, title, embedding, text in zip(batch_release_year, batch_titles, batch_embeddings_cpu, batch_texts)\n",
    "    ]\n",
    "    \n",
    "    # Insert into Milvus\n",
    "    collection.insert(records)\n",
    "\n",
    "# Flush to Milvus to ensure all data is written\n",
    "collection.flush()\n",
    "\n",
    "collection.create_index(field_name=\"plot_embedding\", index_params={\"index_type\": \"IVF_FLAT\", \"metric_type\": \"L2\", \"params\": {\"nlist\": 100}})\n",
    "\n",
    "print(f\"Inserted {len(df)} records into Milvus.\")\n",
    "\n",
    "sample_plot = \"A bartender is working at a saloon, serving drinks to customers. After he fills a stereotypically Irish man's bucket with beer, Carrie Nation and her followers burst inside.\"\n",
    "print(\"Plot Embedding:\",  get_batch_embeddings(sample_plot))\n",
    "\n",
    "# Takes about 23 minutes with Apple silicon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result ID 453300591135315260 is out of bounds for the DataFrame.\n",
      "Result ID 453300591135328003 is out of bounds for the DataFrame.\n",
      "Result ID 453300591135325278 is out of bounds for the DataFrame.\n",
      "Result ID 453300591135327129 is out of bounds for the DataFrame.\n",
      "Result ID 453300591135317514 is out of bounds for the DataFrame.\n"
     ]
    }
   ],
   "source": [
    "def search_similar_plots(plot_text, top_k=5):\n",
    "    # Convert the input plot to an embedding\n",
    "    embedding = get_batch_embeddings([plot_text])  # Pass as a list to get embedding\n",
    "    \n",
    "    # Move embedding to CPU and convert to numpy array\n",
    "    embedding_cpu = embedding.cpu().numpy().squeeze()  # Squeeze to remove extra dimensions\n",
    "    \n",
    "    # Convert numpy array to a list of floats\n",
    "    embedding_list = embedding_cpu.tolist()\n",
    "    \n",
    "    # Perform similarity search\n",
    "    results = collection.search(\n",
    "        data=[embedding_list],  # Ensure it's a list of floats\n",
    "        anns_field=\"plot_embedding\",\n",
    "        param={\"metric_type\": \"L2\"},  # Use L2 (Euclidean distance) or the metric you set\n",
    "        limit=top_k\n",
    "    )\n",
    "    \n",
    "    for result in results[0]:\n",
    "        # Assuming that 'id' was saved in Milvus with the record and matches the DataFrame index\n",
    "        original_id = result.id\n",
    "        \n",
    "        # Safely handle out-of-bounds issues\n",
    "        if original_id < len(df):\n",
    "            plot_text = df.iloc[original_id]['Plot']\n",
    "            title = df.iloc[original_id]['Title']\n",
    "            release_year = df.iloc[original_id]['Release Year']\n",
    "            print(f\"Title: {title}, Release Year: {release_year}\")\n",
    "            print(f\"Original Plot: {plot_text}\")\n",
    "            print(f\"Score: {result.distance}\")\n",
    "        else:\n",
    "            print(f\"Result ID {original_id} is out of bounds for the DataFrame.\")\n",
    "\n",
    "# Load the collection before searching\n",
    "collection.load()\n",
    "\n",
    "# Example query\n",
    "search_similar_plots(\"A group of people go on an adventure to find treasure.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO notes:\n",
    "\n",
    "[] WIP: Figure out why 'Inserted 34886 records into Milvus.' and '134164 data/wiki_movie_plots_deduped.csv' records in dataset\n",
    "\n",
    "[] Use running code outside of development setup in this notwbook from standalone script imports"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ragtime-5o5M5Lb8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
