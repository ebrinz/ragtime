{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG / HyDE Example with Mistral Instruct 7b and Milvus DB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports dependencies and Mistral LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:  33%|███▎      | 1/3 [00:14<00:29, 14.77s/it]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "MPS backend out of memory (MPS allocated: 36.23 GB, other allocations: 1.88 MB, max allowed: 36.27 GB). Tried to allocate 112.00 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmps\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mbackends\u001b[38;5;241m.\u001b[39mmps\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing device: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 14\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat16\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     19\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Test LLM\u001b[39;00m\n\u001b[1;32m     23\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat happens in the movie Inception?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/ragtime-5o5M5Lb8/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:564\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    570\u001b[0m )\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/ragtime-5o5M5Lb8/lib/python3.11/site-packages/transformers/modeling_utils.py:4014\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4004\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   4005\u001b[0m         torch\u001b[38;5;241m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[1;32m   4007\u001b[0m     (\n\u001b[1;32m   4008\u001b[0m         model,\n\u001b[1;32m   4009\u001b[0m         missing_keys,\n\u001b[1;32m   4010\u001b[0m         unexpected_keys,\n\u001b[1;32m   4011\u001b[0m         mismatched_keys,\n\u001b[1;32m   4012\u001b[0m         offload_index,\n\u001b[1;32m   4013\u001b[0m         error_msgs,\n\u001b[0;32m-> 4014\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4015\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4016\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4017\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloaded_state_dict_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# XXX: rename?\u001b[39;49;00m\n\u001b[1;32m   4018\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresolved_archive_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4019\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4020\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4021\u001b[0m \u001b[43m        \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4022\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_fast_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_fast_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4023\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4024\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4025\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4026\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4027\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4028\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4029\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4030\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgguf_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgguf_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4031\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4033\u001b[0m \u001b[38;5;66;03m# make sure token embedding weights are still tied if needed\u001b[39;00m\n\u001b[1;32m   4034\u001b[0m model\u001b[38;5;241m.\u001b[39mtie_weights()\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/ragtime-5o5M5Lb8/lib/python3.11/site-packages/transformers/modeling_utils.py:4502\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[0;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_modules, gguf_path)\u001b[0m\n\u001b[1;32m   4498\u001b[0m                 set_module_tensor_to_device(\n\u001b[1;32m   4499\u001b[0m                     model_to_load, key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch\u001b[38;5;241m.\u001b[39mempty(\u001b[38;5;241m*\u001b[39mparam\u001b[38;5;241m.\u001b[39msize(), dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m   4500\u001b[0m                 )\n\u001b[1;32m   4501\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 4502\u001b[0m         new_error_msgs, offload_index, state_dict_index \u001b[38;5;241m=\u001b[39m \u001b[43m_load_state_dict_into_meta_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4503\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel_to_load\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4504\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4505\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstart_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4506\u001b[0m \u001b[43m            \u001b[49m\u001b[43mexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4507\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4508\u001b[0m \u001b[43m            \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4509\u001b[0m \u001b[43m            \u001b[49m\u001b[43moffload_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4510\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_dict_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate_dict_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4511\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_dict_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate_dict_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4512\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4513\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4514\u001b[0m \u001b[43m            \u001b[49m\u001b[43mis_safetensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_safetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4515\u001b[0m \u001b[43m            \u001b[49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4516\u001b[0m \u001b[43m            \u001b[49m\u001b[43munexpected_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4517\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4518\u001b[0m         error_msgs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m new_error_msgs\n\u001b[1;32m   4519\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   4520\u001b[0m     \u001b[38;5;66;03m# Sharded checkpoint or whole but low_cpu_mem_usage==True\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/ragtime-5o5M5Lb8/lib/python3.11/site-packages/transformers/modeling_utils.py:973\u001b[0m, in \u001b[0;36m_load_state_dict_into_meta_model\u001b[0;34m(model, state_dict, start_prefix, expected_keys, device_map, offload_folder, offload_index, state_dict_folder, state_dict_index, dtype, hf_quantizer, is_safetensors, keep_in_fp32_modules, unexpected_keys, pretrained_model_name_or_path)\u001b[0m\n\u001b[1;32m    970\u001b[0m         param_device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_local_dist_rank_0() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    972\u001b[0m     \u001b[38;5;66;03m# For backward compatibility with older versions of `accelerate` and for non-quantized params\u001b[39;00m\n\u001b[0;32m--> 973\u001b[0m     \u001b[43mset_module_tensor_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_device\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mset_module_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    974\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    975\u001b[0m     hf_quantizer\u001b[38;5;241m.\u001b[39mcreate_quantized_param(model, param, param_name, param_device, state_dict, unexpected_keys)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/ragtime-5o5M5Lb8/lib/python3.11/site-packages/accelerate/utils/modeling.py:329\u001b[0m, in \u001b[0;36mset_module_tensor_to_device\u001b[0;34m(module, tensor_name, device, value, dtype, fp16_statistics, tied_params_map)\u001b[0m\n\u001b[1;32m    327\u001b[0m             module\u001b[38;5;241m.\u001b[39m_parameters[tensor_name] \u001b[38;5;241m=\u001b[39m param_cls(new_value, requires_grad\u001b[38;5;241m=\u001b[39mold_value\u001b[38;5;241m.\u001b[39mrequires_grad)\n\u001b[1;32m    328\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m--> 329\u001b[0m     new_value \u001b[38;5;241m=\u001b[39m \u001b[43mvalue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    331\u001b[0m     new_value \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(value, device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: MPS backend out of memory (MPS allocated: 36.23 GB, other allocations: 1.88 MB, max allowed: 36.27 GB). Tried to allocate 112.00 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure)."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "home_dir = os.getenv(\"HOME\")\n",
    "model_name = f'{home_dir}/ext-gits/Mistral-7B-Instruct-v0.3'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': tokenizer.eos_token})\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map={\"\": device},\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Test LLM\n",
    "\n",
    "prompt = \"What happens in the movie Inception?\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        max_length=50,\n",
    "        num_return_sequences=1,\n",
    "        do_sample=True,\n",
    "        top_p=0.95,\n",
    "        top_k=50,\n",
    "        pad_token_id=tokenizer.pad_token_id\n",
    "    )\n",
    "\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34886\n",
      "Number of completely empty rows: 0\n",
      "Total lines in the file: 34887\n",
      "The longest plot is at index 26064 with length 36773 characters.\n",
      "Longest Plot: After a brief introduction to some of the main characters of the story, the beginning sees a group of Rishis, led by Vishvamitra, performing a Yajna in a forest not far from Ayodhya, the Capital of the Kingdom of Kosala. This Yajna, like several before it, is interrupted and destroyed by a group of flying demons led by Ravana's Mama(Uncle/Mother's Brother) Maricha. After seeing yet another Yajna destroyed, a despondent Vishvamitra appeals to Lord Vishnu for salvation. Vishnu appears in a spiritu...\n",
      "non_nan_rows_coun 34886\n",
      "nan_counts Series([], dtype: int64)\n",
      "                                                Plot\n",
      "0  A bartender is working at a saloon, serving dr...\n",
      "1  The moon, painted with a smiling face hangs ov...\n",
      "2  The film, just over a minute long, is composed...\n",
      "3  Lasting just 61 seconds and consisting of two ...\n",
      "4  The earliest known adaptation of the classic f...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "df = None\n",
    "# Load the dataset\n",
    "df = pd.read_csv('data/wiki_movie_plots_deduped.csv')\n",
    "\n",
    "print(df.shape[0])\n",
    "\n",
    "empty_rows = df[df.isnull().all(axis=1)]\n",
    "print(f\"Number of completely empty rows: {empty_rows.shape[0]}\")\n",
    "\n",
    "with open('data/wiki_movie_plots_deduped.csv', 'r') as file:\n",
    "    reader = csv.reader(file)\n",
    "    total_lines = sum(1 for row in reader)\n",
    "\n",
    "print(f\"Total lines in the file: {total_lines}\")\n",
    "\n",
    "# Ensure there are no NaNs in the 'Plot' column\n",
    "df = df[['Plot']].dropna()\n",
    "\n",
    "# Find the longest plot and its length\n",
    "longest_plot = df['Plot'].apply(len).idxmax()  # Find the index of the longest plot\n",
    "longest_plot_text = df['Plot'].iloc[longest_plot]  # Get the longest plot text\n",
    "longest_plot_length = len(longest_plot_text)  # Get the length of the longest plot text\n",
    "\n",
    "print(f\"The longest plot is at index {longest_plot} with length {longest_plot_length} characters.\")\n",
    "print(f\"Longest Plot: {longest_plot_text[:500]}...\")\n",
    "\n",
    "non_nan_rows_count = df.dropna().shape[0]\n",
    "print(\"non_nan_rows_coun\", non_nan_rows_count) # 34886\n",
    "\n",
    "# need to change logic to accomodate NaNs\n",
    "\n",
    "# Get the count of NaN values for each column\n",
    "nan_counts = df.isna().sum()\n",
    "nan_counts = nan_counts[nan_counts > 0]\n",
    "print(\"nan_counts\", nan_counts)\n",
    "\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "row_count = df.shape[0]\n",
    "row_count\n",
    "\n",
    "df = df.drop(df.index)\n",
    "\n",
    "row_count = df.shape[0]\n",
    "row_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Milvus connected: True\n",
      "Collection 'wiki_movie_plots' dropped.\n",
      "Milvus collection schema created successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1729629804.462455 6676211 fork_posix.cc:77] Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# run milvis locally\n",
    "# run: docker-compose up -d\n",
    "\n",
    "# data is ..data/wiki_movie_plots_deduped.csv is from\n",
    "# https://www.kaggle.com/datasets/jrobischon/wikipedia-movie-plots\n",
    "\n",
    "from pymilvus import connections, CollectionSchema, DataType, FieldSchema, Collection, utility\n",
    "\n",
    "connections.connect(alias=\"default\", host=\"127.0.0.1\", port=\"19530\")\n",
    "\n",
    "print(\"Milvus connected:\", connections.has_connection(alias=\"default\"))\n",
    "\n",
    "collection_name = \"wiki_movie_plots\"\n",
    "dim = 768  # Dimensions for the vector embeddings\n",
    "\n",
    "# Check if the collection exists and drop it if it does\n",
    "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!   WILL DROP\n",
    "# if utility.has_collection(collection_name):\n",
    "#     collection = Collection(collection_name)\n",
    "#     collection.drop()\n",
    "#     print(f\"Collection '{collection_name}' dropped.\")\n",
    "\n",
    "\n",
    "fields = [\n",
    "    FieldSchema(name=\"id\", dtype=DataType.INT64, is_primary=True),  # Primary key field\n",
    "    FieldSchema(name=\"plot_embedding\", dtype=DataType.FLOAT_VECTOR, dim=dim),\n",
    "    FieldSchema(name=\"plot_text\", dtype=DataType.VARCHAR, max_length=40000),  # To store original plot text\n",
    "    FieldSchema(name=\"release_year\", dtype=DataType.INT64),\n",
    "    FieldSchema(name=\"title\", dtype=DataType.VARCHAR, max_length=255),\n",
    "]\n",
    "\n",
    "# Create the schema and collection\n",
    "schema = CollectionSchema(fields, description=\"Wikipedia Movie Plots with vector embeddings and original plot text\")\n",
    "collection = Collection(name=collection_name, schema=schema)\n",
    "\n",
    "print(\"Milvus collection schema created successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_embeddings(batch_texts, model, tokenizer, device, remove_token_type_ids=False, mean_pooling=True):\n",
    "    \"\"\"\n",
    "    Generalized function to get embeddings for different use cases.\n",
    "    \n",
    "    Parameters:\n",
    "    - batch_texts: List of input texts to be embedded.\n",
    "    - model: The pre-trained model used for generating embeddings.\n",
    "    - tokenizer: The tokenizer corresponding to the model.\n",
    "    - device: The device to run the model on ('cpu', 'cuda', 'mps').\n",
    "    - remove_token_type_ids: Set to True if 'token_type_ids' should be removed from input dict (e.g., for causal LMs like GPT/Mistral).\n",
    "    - mean_pooling: Set to True if you want to perform mean pooling over the output embeddings.\n",
    "    \n",
    "    Returns:\n",
    "    - embeddings: The embeddings for the input batch.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(batch_texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n",
    "    if remove_token_type_ids and 'token_type_ids' in inputs:\n",
    "        del inputs['token_type_ids']\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    if mean_pooling:\n",
    "        return outputs.last_hidden_state.mean(dim=1)\n",
    "    else:\n",
    "        return outputs.logits.mean(dim=1)\n",
    "\n",
    "\n",
    "import re\n",
    "\n",
    "def clean_plot(plot):\n",
    "    plot = re.sub(r'\\[.*?\\]', '', plot)  # Remove anything in square brackets\n",
    "    plot = re.sub(r'\\s+', ' ', plot)     # Replace multiple spaces with a single space\n",
    "    plot = plot.strip()                  # Remove leading/trailing spaces\n",
    "    return plot.lower()                  # Convert to lowercase\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from pymilvus import Collection\n",
    "\n",
    "def search_similar_plots(plot_text, *args, top_k=5, **kwargs):\n",
    "\n",
    "    collection = Collection(\"wiki_movie_plots\")\n",
    "\n",
    "    embedding = get_batch_embeddings([plot_text], *args, **kwargs)\n",
    "    embedding_cpu = embedding.cpu().numpy().squeeze()\n",
    "    embedding_list = embedding_cpu.tolist()\n",
    "    \n",
    "    results = collection.search(\n",
    "        data=[embedding_list],\n",
    "        anns_field=\"plot_embedding\",\n",
    "        param={\"metric_type\": \"L2\"},\n",
    "        limit=top_k,\n",
    "        output_fields=[\"id\"]\n",
    "    )\n",
    "\n",
    "    # wtf\n",
    "    csv_file_path = 'data/wiki_movie_plots_deduped.csv'\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "    df = df[['Release Year', 'Title', 'Plot']].dropna()\n",
    "    \n",
    "    response = []\n",
    "    for result in results[0]:\n",
    "        original_id = result.entity.get(\"id\")\n",
    "        \n",
    "        if original_id < len(df):\n",
    "            plot_text = df.iloc[original_id]['Plot']\n",
    "            title = df.iloc[original_id]['Title']\n",
    "            release_year = df.iloc[original_id]['Release Year']\n",
    "            response.append({\n",
    "                \"title\": title,\n",
    "                \"release_year\": release_year,\n",
    "                \"plot_text\": plot_text,\n",
    "                \"score\": result.distance\n",
    "            })\n",
    "        else:\n",
    "            print(f\"Result ID {original_id} is out of bounds for the DataFrame.\")\n",
    "\n",
    "    return response\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from pymilvus import Collection\n",
    "\n",
    "def insert_embeddings_to_milvus(\n",
    "    csv_file_path, \n",
    "    collection, \n",
    "    embeddings_model_name, \n",
    "    batch_size=32, \n",
    "    device=None\n",
    "):\n",
    "    \n",
    "    df = pd.read_csv(csv_file_path)\n",
    "    df = df[['Release Year', 'Title', 'Plot']].dropna()\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(embeddings_model_name)\n",
    "    model = AutoModel.from_pretrained(embeddings_model_name)\n",
    "\n",
    "    device = device or torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    for i in range(0, len(df), batch_size):\n",
    "        batch_texts = [clean_plot(text) for text in df['Plot'].iloc[i:i+batch_size].tolist()]\n",
    "        batch_titles = df['Title'].iloc[i:i+batch_size].tolist()\n",
    "        batch_release_year = df['Release Year'].iloc[i:i+batch_size].tolist()\n",
    "        batch_ids = df.index[i:i+batch_size].tolist() \n",
    "        batch_embeddings = get_batch_embeddings(batch_texts, model, tokenizer, device, mean_pooling=True)\n",
    "        batch_embeddings_cpu = batch_embeddings.cpu().numpy()\n",
    "\n",
    "        # Prepare records\n",
    "        records = [\n",
    "            {\n",
    "                \"id\": id_value,\n",
    "                \"release_year\": release_year,\n",
    "                \"title\": title,\n",
    "                \"plot_embedding\": embedding.tolist(),  # Convert to list for insertion\n",
    "                \"plot_text\": text\n",
    "            }\n",
    "            for id_value, release_year, title, embedding, text in zip(batch_ids, batch_release_year, batch_titles, batch_embeddings_cpu, batch_texts)\n",
    "        ]\n",
    "        collection.insert(records)\n",
    "\n",
    "    # Flush and create index\n",
    "    collection.flush()\n",
    "    collection.create_index(field_name=\"plot_embedding\", index_params={\"index_type\": \"IVF_FLAT\", \"metric_type\": \"L2\", \"params\": {\"nlist\": 100}})\n",
    "    collection.load()\n",
    "\n",
    "    # Clear memory\n",
    "    torch.mps.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insert data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "from pymilvus import Collection\n",
    "\n",
    "\n",
    "\n",
    "# csv_file_path = 'data/wiki_movie_plots_deduped.csv'\n",
    "# embeddings_model_name = \"sentence-transformers/bert-base-nli-mean-tokens\"\n",
    "# collection = Collection(\"wiki_movie_plots\")\n",
    "\n",
    "# insert_embeddings_to_milvus(csv_file_path, collection, embeddings_model_name)\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doublecheck ingestion row count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records in the collection: 34886\n",
      "Sample records: data: ['{\\'id\\': 1, \\'plot_text\\': \"the moon, painted with a smiling face hangs over a park at night. a young couple walking past a fence learn on a railing and look up. the moon smiles. they embrace, and the moon\\'s smile gets bigger. they then sit down on a bench by a tree. the moon\\'s view is blocked, causing him to frown. in the last scene, the man fans the woman with his hat because the moon has left the sky and is perched over her shoulder to see everything better.\"}', \"{'id': 2, 'plot_text': 'the film, just over a minute long, is composed of two shots. in the first, a girl sits at the base of an altar or tomb, her face hidden from the camera. at the center of the altar, a viewing portal displays the portraits of three u.s. presidents—abraham lincoln, james a. garfield, and william mckinley—each victims of assassination. in the second shot, which runs just over eight seconds long, an assassin kneels feet of lady justice.'}\", '{\\'id\\': 3, \\'plot_text\\': \\'lasting just 61 seconds and consisting of two shots, the first shot is set in a wood during winter. the actor representing then vice-president theodore roosevelt enthusiastically hurries down a hillside towards a tree in the foreground. he falls once, but rights himself and cocks his rifle. two other men, bearing signs reading \"his photographer\" and \"his press agent\" respectively, follow him into the shot; the photographer sets up his camera. \"teddy\" aims his rifle upward at the tree and fells what appears to be a common house cat, which he then proceeds to stab. \"teddy\" holds his prize aloft, and the press agent takes notes. the second shot is taken in a slightly different part of the wood, on a path. \"teddy\" rides the path on his horse towards the camera and out to the left of the shot, followed closely by the press agent and photographer, still dutifully holding their signs.\\'}', '{\\'id\\': 4, \\'plot_text\\': \"the earliest known adaptation of the classic fairytale, this films shows jack trading his cow for the beans, his mother forcing him to drop them in the front yard, and beig forced upstairs. as he sleeps, jack is visited by a fairy who shows him glimpses of what will await him when he ascends the bean stalk. in this version, jack is the son of a deposed king. when jack wakes up, he finds the beanstalk has grown and he climbs to the top where he enters the giant\\'s home. the giant finds jack, who narrowly escapes. the giant chases jack down the bean stalk, but jack is able to cut it down before the giant can get to safety. he falls and is killed as jack celebrates. the fairy then reveals that jack may return home as a prince.\"}', '{\\'id\\': 5, \\'plot_text\\': \\'alice follows a large white rabbit down a \"rabbit-hole\". she finds a tiny door. when she finds a bottle labeled \"drink me\", she does, and shrinks, but not enough to pass through the door. she then eats something labeled \"eat me\" and grows larger. she finds a fan when enables her to shrink enough to get into the \"garden\" and try to get a \"dog\" to play with her. she enters the \"white rabbit\\\\\\'s tiny house,\" but suddenly resumes her normal size. in order to get out, she has to use the \"magic fan.\" she enters a kitchen, in which there is a cook and a woman holding a baby. she persuades the woman to give her the child and takes the infant outside after the cook starts throwing things around. the baby then turns into a pig and squirms out of her grip. \"the duchess\\\\\\'s cheshire cat\" appears and disappears a couple of times to alice and directs her to the mad hatter\\\\\\'s \"mad tea-party.\" after a while, she leaves. the queen invites alice to join the \"royal procession\": a parade of marching playing cards and others headed by the white rabbit. when alice \"unintentionally offends the queen\", the latter summons the \"executioner\". alice \"boxes the ears\", then flees when all the playing cards come for her. then she wakes up and realizes it was all a dream.\\'}'] \n"
     ]
    }
   ],
   "source": [
    "## Run this to connect to db if already performed data insert\n",
    "## ie: you run out or memory and restart the kernel and need to reconnect to db\n",
    "\n",
    "from pymilvus import connections, Collection\n",
    "\n",
    "# Connect to Milvus\n",
    "connections.connect(alias=\"default\", host=\"localhost\", port=\"19530\")\n",
    "collection = Collection(\"wiki_movie_plots\")\n",
    "collection.load()\n",
    "\n",
    "num_records = collection.num_entities\n",
    "print(f\"Number of records in the collection: {num_records}\")\n",
    "\n",
    "if num_records > 0:\n",
    "    sample_records = collection.query(expr=\"id >= 1\", output_fields=[\"id\", \"plot_text\"], limit=5)\n",
    "    print(\"Sample records:\", sample_records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:18<00:00,  6.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "got loaded\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModel\n",
    "import os, torch\n",
    "\n",
    "home_dir = os.getenv(\"HOME\")\n",
    "\n",
    "rag_model_name = f'{home_dir}/ext-gits/Mistral-7B-Instruct-v0.3'\n",
    "rag_tokenizer = AutoTokenizer.from_pretrained(rag_model_name)\n",
    "rag_device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "rag_model = AutoModelForCausalLM.from_pretrained(\n",
    "    rag_model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map={\"\": device},\n",
    "    trust_remote_code=True\n",
    "    )\n",
    "\n",
    "if rag_tokenizer.pad_token is None:\n",
    "    rag_tokenizer.add_special_tokens({'pad_token':rag_tokenizer.eos_token})\n",
    "\n",
    "rag_model.to(device)\n",
    "\n",
    "\n",
    "db_model_name = \"sentence-transformers/bert-base-nli-mean-tokens\"\n",
    "db_tokenizer = AutoTokenizer.from_pretrained(db_model_name)\n",
    "db_model = AutoModel.from_pretrained(db_model_name)\n",
    "\n",
    "db_device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "db_model.to(device)\n",
    "\n",
    "print('got loaded')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "outcome:\n",
      "\n",
      "\n",
      "Givent the following descriptions a flight turns dangerous when the passengers find a ton of snakes on boardA deranged man calling himself James Pettis (Val Kilmer) approaches The Grand Rapids Press demanding that it publish his predictions about the upcoming demise of civilization due to the conditions of global warming, warning that he has trapped a group of six people in a Turkish-style steamroom to demonstrate the effects of this environment on humans.[1] A local police detective Mancini (Armand Assante) tries to get Pettis to reveal information that will help him confirm the truth of his threat and to rescue the hostages, but over the course of the interrogation begins to suspect that either Pettis' story is a delusional hoax, or that the steamroom killing has already taken place.\n",
      "As Pettis describes developments in the streamroom to Mancini, the scene is shown of three men and three women meeting in the steamroom of a luxury hotel as part of an online dating promotion, then being locked in together. When they discover that they have been locked in, they react badly: Frank (Quinn Duffy) becomes abusive to Jessie (Eve Mauro), and is killed in her defense by openly neurotic Margaret (Cordelia Reynolds). Jessie is killed with a nail gun by an unseen assailant when she pokes her head through the small window in the steamroom door; Christopher (Patrick Muldoon) is injured in the hand with a nail as the window is boarded over from outside. Margaret becomes agitated and commits suicide. Grant (Eric Roberts) is bludgeoned by Catherine (Megan Brown) after he accuses her and Christopher of being allies of the perpetrators and repeatedly holds her head underwater.\n",
      "Mancini's call to Pettis' psychiatrist finally brings staff from the local state psychiatric hospital, from which Pettis recently escaped. It is revealed that Christopher and Catherine are staff at this facility, and are unhappy with Pettis for going to the news media and police with this story.\n",
      "\n",
      "A college student Seema sees in her nightmare that a horrible looking man wearing steel claw gloves attacks her. She wakes up to find that she has real wounds on her arm. Later her friend Anita too sees the same nightmare and finds real wounds on her arm. Anita tells her parents about her nightmare. Her father, who is a policeman, refuses to believe it. Later, Seema is attacked again in her dream and she dies because of the wounds. Her boyfriend is put in police lockup where he sees the horrible man who makes snakes appear by magic. The boyfriend dies of snake-bite. Later, Anita and her mother catch Anita's father taking out a metal clawed glove from his drawer, so he is forced to reveal the secret that an evil magician Shakal had been kidnapping children and sacrificing them to increase his evil powers. Seven years back, Shakal had killed Anita's sister too. Finally, Shakal kidnaps Anita. As her father knows Shakal's place, he arrives with Anita's boyfriend and they manage to kill him and save Anita., create a new plotline for a movie/tv episode called \"The Steamroom Killer\" or \"The Hostages\" that combines elements of psychological thriller, horror, and suspense genres.\n",
      "\n",
      "In this plotline, a former psychiatrist named James\n"
     ]
    }
   ],
   "source": [
    "def plots_query(query, *args, **kwargs):\n",
    "    search_results = search_similar_plots(query, top_k=2, *args, **kwargs)\n",
    "    context_texts = [res['plot_text'] for res in search_results]\n",
    "    return query + \"\\n\\n\".join(context_texts)\n",
    "\n",
    "def generate_response(combined_texts, *args, max_new_tokens=50):\n",
    "    model, tokenizer, device = args\n",
    "\n",
    "    new_prompt = f'Givent the following descriptions {combined_texts}, create a new plotline'\n",
    "\n",
    "    inputs = tokenizer(new_prompt, return_tensors=\"pt\", padding=True).to(device)\n",
    "\n",
    "    print('')\n",
    "    print('')\n",
    "    print('outcome:')\n",
    "    print('')\n",
    "    print('')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            # max_length=500,\n",
    "            num_return_sequences=1,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            top_p=0.95,\n",
    "            top_k=50,\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "\n",
    "combined_query = plots_query(\n",
    "    \"a flight turns dangerous when the passengers find a ton of snakes on board\",\n",
    "    db_model, \n",
    "    db_tokenizer, \n",
    "    db_device, \n",
    "    mean_pooling=True,\n",
    "    remove_token_type_ids=True\n",
    "    )\n",
    "\n",
    "outcome = generate_response(\n",
    "    combined_query,\n",
    "    rag_model, \n",
    "    rag_tokenizer, \n",
    "    rag_device, \n",
    "    )\n",
    "\n",
    "print(outcome)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODOS:\n",
    "\n",
    "[] Refactor rag implementation\n",
    "\n",
    "## \n",
    "\n",
    "[] Adjust mistral params\n",
    "\n",
    "##\n",
    "\n",
    "[] Create HyDE\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ragtime-5o5M5Lb8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
